#!/bin/bash
#SBATCH -n 1                # Number of cores
#SBATCH -N 1                # Ensure that all cores are on one machine
#SBATCH -t 0-12:00          # Runtime in D-HH:MM, minimum of 10 minutes
#SBATCH -p huce_bigmem  # Partition to submit to
#SBATCH --mem=100000         # Memory pool for all cores (see also --mem-per-cpu)
#SBATCH -o prep_%j.out    # File to which STDOUT will be written, %j inserts jobid       
#SBATCH -e prep_%j.err    # File to which STDERR will be written, %j inserts jobid

source ../environments/cheereio.env #This is specific to the Harvard cluster; rewrite for yours
eval "$(conda shell.bash hook)"

conda activate $(jq -r ".CondaEnv" ../ens_config.json) 
python -u postprocess_workflow.py "histprocess" >> postprocess_prep_out.txt
conda deactivate 
bash make_animations.sh
conda activate $(jq -r ".CondaEnv" ../ens_config.json)
python counts_prep.py
conda deactivate 
conda activate $(jq -r ".AnimationEnv" ../ens_config.json)
python counts_animator.py
conda deactivate