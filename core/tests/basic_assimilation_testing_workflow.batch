#!/bin/bash
#SBATCH -n 1                # Number of cores
#SBATCH -N 1                # Ensure that all cores are on one machine
#SBATCH -t 0-01:00          # Runtime in D-HH:MM, minimum of 10 minutes
#SBATCH -p huce_intel  # Partition to submit to
#SBATCH --mem=8000         # Memory pool for all cores (see also --mem-per-cpu)
#SBATCH -o basic_testing_%j.out    # File to which STDOUT will be written, %j inserts jobid       
#SBATCH -e basic_testing_%j.err    # File to which STDERR will be written, %j inserts jobid

#This testing workflow verifies that the most important aspects of the assimilation module are working properly.
#It checks that state vectors are formed and subsetted properly, and that assimilation calculations are correct.
#It also verifies that satellite operators are working as expected and that data is being passed across modules correctly.

#However, not every scenario can be tested by this workflow, so please be careful!
#This workflow also does not test the full CHEEREIO run, which involves running GEOS-Chem. It only works on assimilation.

#Run this workflow every time you make modifications to the assimilation workflow, just to make sure you didn't break it!

source ../../environments/cheereio.env #This is specific to the Harvard cluster; rewrite for yours
eval "$(conda shell.bash hook)"

conda activate $(jq -r ".CondaEnv" ../../ens_config.json) 
python prep_testing_env.py
pytest